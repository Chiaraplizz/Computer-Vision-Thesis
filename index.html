<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="robots" content="noindex, nofollow">
  <meta charset="UTF-8">
  <title>Computer Vision Thesis</title>
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap" rel="stylesheet">
  <style>
    body {
      margin: 0;
      font-family: 'Open Sans', sans-serif;
      background: #f9f9f9;
      color: #333;
    }

    /* Sidebar */
    #sidebar {
      position: fixed;
      top: 0;
      left: 0;
      height: 100%;
      width: 240px;
      background: #0056b3;
      color: white;
      padding-top: 40px;
      overflow-y: auto;
    }
    #sidebar h2 {
      text-align: center;
      font-size: 18px;
      margin-bottom: 20px;
    }
    #sidebar ul {
      list-style: none;
      padding: 0;
      margin: 0;
    }
    #sidebar ul li {
      margin: 10px 0;
    }
    #sidebar ul li a {
      color: white;
      text-decoration: none;
      padding: 8px 20px;
      display: block;
      transition: background 0.3s;
    }
    #sidebar ul li a:hover {
      background: #004494;
      border-radius: 6px;
    }

    /* Sidebar groups */
    .nav-group {
      margin-top: 14px;
    }
    /* works for both <div> and <summary> */
    .nav-group-title {
      padding: 10px 20px;
      font-weight: 700;
      font-size: 13px;
      text-transform: uppercase;
      letter-spacing: 0.5px;
      opacity: 0.95;
    }
    .nav-sublist li a {
      padding-left: 34px; /* indent sub-items */
      font-size: 14px;
      opacity: 0.98;
    }

    /* Collapsible sidebar group (Old Proposals) */
    .nav-details {
      margin-top: 14px;
    }
    .nav-details > summary.nav-group-title {
      list-style: none;
      cursor: pointer;
      user-select: none;
      position: relative;
      padding-right: 34px; /* space for arrow */
      transition: background 0.3s;
    }
    .nav-details > summary::-webkit-details-marker {
      display: none;
    }
    .nav-details > summary.nav-group-title::after {
      content: "â–¸";
      position: absolute;
      right: 20px;
      top: 50%;
      transform: translateY(-50%);
      opacity: 0.9;
    }
    .nav-details[open] > summary.nav-group-title::after {
      content: "â–¾";
    }
    .nav-details > summary.nav-group-title:hover {
      background: #004494;
      border-radius: 6px;
    }

    /* Main content */
    #content {
      margin-left: 260px; /* space for sidebar */
      padding: 40px;
      max-width: 900px;
    }
    header {
      margin-bottom: 40px;
    }
    h1 {
      font-size: 32px;
      color: #222;
      margin-bottom: 10px;
    }
    h2 {
      color: #0056b3;
      margin-top: 40px;
    }
    .section {
      background: white;
      padding: 25px;
      border-radius: 12px;
      box-shadow: 0 2px 5px rgba(0,0,0,0.1);
      margin-bottom: 40px;
    }
    .requirements {
      font-weight: bold;
      color: #0056b3;
    }
    .collaborators {
      font-style: italic;
      color: #444;
    }
    ul.references {
      margin-top: 15px;
      padding-left: 20px;
      font-size: 14px;
      color: #555;
    }
    footer {
      text-align: center;
      padding: 20px;
      margin-top: 40px;
      font-size: 14px;
      color: #666;
    }

    img.placeholder {
      display: block;
      margin: 15px auto;
      max-width: 80%; /* smaller than the text width */
      height: auto;
      border: 2px dashed #ccc;
      border-radius: 8px;
      padding: 10px;
      background: #fafafa;
    }

    /* Collapsible block in main content (Old Proposals) */
    .main-details {
      margin-bottom: 40px;
    }
    .main-details > summary.section-toggle {
      list-style: none;
      cursor: pointer;
      user-select: none;
      font-size: 26px;
      font-weight: 700;
      color: #0056b3;
      margin: 10px 0 20px 0;
      position: relative;
      padding-right: 28px; /* space for arrow */
    }
    .main-details > summary.section-toggle::-webkit-details-marker {
      display: none;
    }
    .main-details > summary.section-toggle::after {
      content: "â–¾";
      position: absolute;
      right: 0;
      top: 50%;
      transform: translateY(-50%);
      opacity: 0.9;
    }
    .main-details:not([open]) > summary.section-toggle::after {
      content: "â–¸";
    }
  </style>
</head>
<body>

  <!-- Sidebar -->
  <nav id="sidebar">
    <h2>Sections</h2>
    <ul>
      <li><a href="#intro">Intro</a></li>

      <!-- Old Proposals: collapsible (closed by default) -->
      <li class="nav-group">
        <details class="nav-details">
          <summary class="nav-group-title">Old Proposals</summary>
          <ul class="nav-sublist">
            <li><a href="#old-proposals">Overview</a></li>
            <li><a href="#section1">Video QA</a></li>
            <li><a href="#section2">Neurosymbolic</a></li>
            <li><a href="#section3">World Models</a></li>
            <li><a href="#section4">4D Scene</a></li>
            <li><a href="#section5">Brain Functioning</a></li>
            <li><a href="#section6">Ego-4D / Ego-Exo-4D</a></li>
          </ul>
        </details>
      </li>

      <!-- New Proposals: always visible -->
      <li class="nav-group">
        <div class="nav-group-title">New Proposals</div>
        <ul class="nav-sublist">
          <li><a href="#new-proposals">Overview</a></li>
          <li><a href="#proposal1">Proposal 1</a></li>
          <li><a href="#proposal2">Proposal 2</a></li>
        </ul>
      </li>
    </ul>
  </nav>

  <!-- Main content -->
  <div id="content">

    <!-- Intro -->
    <header id="intro">
      <h1>Computer Vision Thesis</h1>
      <p>
        The topics here outlined reflect areas Iâ€™m curious about and motivated to explore. Each contributes to a different piece of a bigger challenge, building AI systems that can perceive, model, and reason about the world through vision in a human-like way. At the same time, Iâ€™m very open to fresh perspectives. You may have better or more novel directions in mind, and Iâ€™d be excited to follow those paths if they open up fun projects! ðŸ˜„
      </p>
      <p><strong>Ps.</strong> This document is evolving, new ideas for research projects might come up! Stay tuned! ðŸ”¥</p>
      <p>
        To express your interest, fill this form:
        <a href="https://forms.office.com/Pages/ResponsePage.aspx?id=erXza7Sfwkea2lEVZRj1L-ucqQXLp61BngrMIMgrk6pUMU5GWVczWU5VUkE0VkFIQjJWR0lEQVg0VS4u" target="_blank">Form</a>
      </p>
      <p>
        For any question, contact me at:
        <a href="mailto:chiara.plizzari@unibocconi.it">chiara.plizzari@unibocconi.it</a>
      </p>
    </header>

    <!-- ========================= -->
    <!-- Old proposals (group)     -->
    <!-- ========================= -->
    <section id="old-proposals">

      <!-- Old proposals: collapsible in main page (CLOSED by default) -->
      <details id="old-proposals-details" class="main-details">
        <summary class="section-toggle">Old Proposals</summary>

        <div class="section">
          <p>
            The following thesis directions are previous proposals. They may still be available, but are grouped separately from newer ideas.
          </p>
        </div>

        <!-- Section 1 -->
        <div id="section1" class="section">
          <h2>1. Towards Robust Video-Question Answering</h2>
          <p class="requirements">Requirements: Python, PyTorch</p>
          <figure style="text-align: center;">
            <img src="images/vqa.png" alt="4D Scene Placeholder" class="placeholder">
            <figcaption style="font-size: 0.9em; color: #555; margin-top: 6px;">Image from [4] </figcaption>
          </figure>
          <p>
            The task of Video Question Answering (Video QA) involves analyzing a video in order to correctly answer a given natural-language question. Recent approaches primarily leverage large vision-language models (VLMs), which have shown strong capabilities in multimodal understanding [5].
          </p>
          <p>
            However, these models face notable limitations. A primary challenge lies in their difficulty in processing long videos due to input token constraints, which restrict the number of frames or segments that can be effectively analyzed [3]. As a result, important portions of the video may be omitted or compressed, leading to incomplete understanding. Beyond this, many models struggle to capture fine-grained temporal dynamics and subtle causal relationships between events, both of which are critical for tasks that require reasoning over sequences rather than isolated frames [2]. Their capacity for long-range reasoning is also limited, making it difficult to integrate information spread across distant segments of a video [3,5].
          </p>
          <p>
            The goal of this thesis is to address one of these limitations by developing methods that enhance the reasoning capabilities of Video QA systems, with the broader goal of building more robust models for real-world video understanding [1,4].
          </p>
          <p><strong>References:</strong></p>
          <ul class="references">
            <li>[1] Taluzzi, Agnese, et al. "From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge." arXiv preprint arXiv:2506.08553 (2025).</li>
            <li>[2] Plizzari, Chiara, et al. "Omnia de egotempo: Benchmarking temporal understanding of multi-modal llms in egocentric videos." Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.</li>
            <li>[3] Liu, Shuming, et al. "BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding." Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.</li>
            <li>[4] Perrett, Toby, et al. "Hd-epic: A highly-detailed egocentric video dataset." Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.</li>
            <li>[5] Zhang, Boqiang, et al. "Videollama 3: Frontier multimodal foundation models for image and video understanding." arXiv preprint arXiv:2501.13106 (2025).</li>
          </ul>
        </div>

        <!-- Section 2 -->
        <div id="section2" class="section">
          <h2>2. Neurosymbolic Approaches for Video Understanding</h2>
          <p class="requirements">Requirements: Python, PyTorch</p>
          <img src="images/neuro.png" alt="4D Scene Placeholder" class="placeholder">
          <p>
            An area of particular interest is the use of neuro-symbolic approaches for video understanding. Instead of reasoning directly over raw pixel data, the question is whether computer vision models, such as large vision-language models (VLMs), can operate effectively on higher-level, structured representations of videos. These representations might include scene graphs, captions, or other symbolic abstractions that capture semantic information from the video. For example, scene graphs are commonly used to model the relationships between entities and actions within a scene [2].
          </p>
          <p>
            By shifting the reasoning process from the video to these structured representations, models may achieve deeper temporal and causal understanding, greater interpretability, and improved scalability for long and complex videos [2].
          </p>
          <p>
            Building on prior research experience, where a previous M.Sc. thesis led to securing first place in the HD-EPIC challenge [1], this thesis investigates how neuro-symbolic approaches can be leveraged for more robust video understanding.
          </p>
          <p><strong>References:</strong></p>
          <ul class="references">
            <li>[1] Taluzzi, Agnese, et al. "From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge." arXiv preprint arXiv:2506.08553 (2025).</li>
            <li>[2] Rodin, I., Furnari, A., Min, K., Tripathi, S., & Farinella, G. M. (2024). "Action scene graphs for long-form understanding of egocentric videos." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 18622-18632).</li>
          </ul>
        </div>

        <!-- Section 3 -->
        <div id="section3" class="section">
          <h2>3. World Models for Video Understanding
            <img src="images/microsoft.png" alt="Microsoft Research Logo" style="height:42px; margin-left:20px; vertical-align:middle;">
          </h2>
          <p class="requirements">Requirements: Python, PyTorch</p>
          <p class="collaborators">Collaborators: <a href="https://gabrielegoletto.github.io/" target="_blank">Gabriele Goletto</a> from Microsoft Research</p>
          <figure style="text-align: center;">
            <img src="images/wm.png" alt="4D Scene Placeholder" class="placeholder">
            <figcaption style="font-size: 0.9em; color: #555; margin-top: 6px;">Image from [6] </figcaption>
          </figure>
          <p>...</p>
        </div>

        <!-- Section 4 -->
        <div id="section4" class="section">
          <h2>4. 4D Scene Understanding for Egocentric Vision</h2>
          <p>...</p>
        </div>

        <!-- Section 5 -->
        <div id="section5" class="section">
          <h2>5. Using Computer Vision to Understand Biological Brain Functioning</h2>
          <p>...</p>
        </div>

        <!-- Section 6 -->
        <div id="section6" class="section">
          <h2>6. Ego-4D / Ego-Exo-4D Challenges</h2>
          <p>...</p>
        </div>

      </details>
    </section>

    <!-- ========================= -->
    <!-- New proposals (group)     -->
    <!-- ========================= -->
    <section id="new-proposals">

      <div class="section">
        <h2>New Proposals</h2>
        <p>
          This section is reserved for new research directions and thesis ideas. Two placeholders are provided below.
        </p>
      </div>

      <div id="proposal1" class="section">
        <h2>Proposal 1. <!-- Title --></h2>
        <p class="requirements">Requirements: <!-- e.g., Python, PyTorch --></p>
        <p class="collaborators">Collaborators: <!-- optional --></p>
      </div>

      <div id="proposal2" class="section">
        <h2>Proposal 2. <!-- Title --></h2>
        <p class="requirements">Requirements: <!-- e.g., Python, PyTorch --></p>
        <p class="collaborators">Collaborators: <!-- optional --></p>
      </div>

    </section>

  </div>

  <footer>
    Â© 2025 Computer Vision Thesis â€” Draft research directions
  </footer>

  <!-- Auto-open Old Proposals in main content ONLY when navigating to its anchors -->
  <script>
    (function () {
      const oldDetails = document.getElementById('old-proposals-details');
      if (!oldDetails) return;

      // Force CLOSED on first load (no matter what)
      oldDetails.open = false;

      function openOldIfNeeded() {
        const hash = location.hash ? location.hash.slice(1) : "";
        if (!hash) return;

        const target = document.getElementById(hash);
        if (!target) return;

        // open if hash is #old-proposals or the target is inside the details
        const shouldOpen = (hash === "old-proposals") || oldDetails.contains(target);
        if (!shouldOpen) return;

        const wasOpen = oldDetails.open;
        oldDetails.open = true;

        // If it was closed, re-scroll after opening so the target is visible.
        if (!wasOpen) {
          setTimeout(() => target.scrollIntoView({ block: "start" }), 0);
        }
      }

      window.addEventListener("hashchange", openOldIfNeeded);
      openOldIfNeeded(); // open only if there's a hash pointing inside
    })();
  </script>

</body>
</html>
